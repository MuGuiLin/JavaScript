<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>大文件分片</title>
    <style>
        h1 {
            text-align: center;
        }

        .box {
            margin: 20px;
            font-size: 32px;
        }
    </style>
</head>

<body>
    <h1>大文件分片（普通版）</h1>
    <hr />
    <div class="box">
        md5：<input type="file" id="file" />
        <h3 id="chunks"></h3>
    </div>
    <div class="box">
        spark-md5：<input type="file" id="file2" />
        <h3 id="chunks2"></h3>
    </div>

    <script src="./md5.js"></script>
    <script>
        const inpfile = document.querySelector('#file');


        // 创建文件分片（通过文件分片内容计算HASH值）
        const createChunk = (file, index, chunkSize) => {
            // console.log(file, index, chunkSize);
            return new Promise((resolve, reject) => {
                const start = index * chunkSize;
                const end = start + chunkSize;
                // 文件切片
                const blob = file.slice(start, end);
                const fileReader = new FileReader();

                // 监听文件分片内容加载完成时，用文件分片内容计算HASH值，但这样做速度太慢了，可以考虑通过抽样的形式的计算HASH值
                fileReader.onload = (e) => {
                    // console.log(e.target.result);
                    resolve({
                        start,
                        end,
                        index,
                        hash: md5(index + e.target.result),
                        blob,
                    });
                };

                // fileReader.onerror = (e) => {
                //     reject(e);
                // };

                // fileReader.onprogress = (e) => {
                //     console.log('切片进度：', e);
                // };

                //  开始读取指定 Blob 或 File 的内容（读取操作完成时，readyState 属性变为 DONE，并触发 loadend 事件）
                fileReader.readAsArrayBuffer(blob);
            })
        };

        // 创建文件分片（通过分片索引计算HASH值）
        const createChunk2 = (file, index, chunkSize) => {
            // console.log(file, index, chunkSize);
            return new Promise((resolve, reject) => {
                const start = index * chunkSize;
                const end = start + chunkSize;
                // 文件切片
                const blob = file.slice(start, end);
                resolve({
                    start,
                    end,
                    index,
                    hash: md5(index),
                    /*
                        在前端进行大文件上传时，计算文件的哈希值通常是为了确保文件的完整性和唯一性。
                        使用循环时的索引（index）来计算哈希值并不是一个合适的方法，
                        因为索引只是一个循环变量，并不能反映文件内容的实际信息。
                        在计算哈希值时，应该使用文件内容的实际信息，而不是索引来计算哈希值。
                        所以 可以用 spark-md5
                    */
                    blob,
                });
            })
        };

        // 定义文件每片的分片大小 5MB
        const CHUNK_SIZE = 1024 * 1024 * 5;

        const cutFile = async (file) => {

            // 计算文件总分片数
            const chunkCount = Math.ceil(file.size / CHUNK_SIZE); // 分片向上取整，如：5.5片就要分6片
            console.log('文件总分片数：', chunkCount);

            console.log('\n文件分片中...！🚀如果文件是存储SSD固态硬盘中的话，读取速度会有非常大的提升哦！！\n\n');

            const chunks = [];
            // 获取文件分片每片的信息
            for (let i = 0; i < chunkCount; i++) {
                // 可能 这里依次等待创建分片的耗时太长了
                // const chunk = await createChunk(file, i, CHUNK_SIZE);
                // chunks.push(chunk);

                // 所以 试先直接添加分片任务，最后用Promise并行创建分片
                // chunks.push(createChunk(file, i, CHUNK_SIZE))
                chunks.push(createChunk2(file, i, CHUNK_SIZE))
            }
            // return chunks;
            // 最后用Promise并行创建分片
            return await Promise.all(chunks);

            // 通过以上的尝试最后速度效果都差不多, 所以可以尝试用多线程worker的方式来处理。
        };


        inpfile.onchange = async (e) => {
            const file = e.target.files[0];
            console.time('耗时');

            const chunks = await cutFile(file);
            document.querySelector('#chunks').innerText = chunks

            console.log('分片结果：', chunks);
        }
    </script>

    <script src="./spark-md5.js"></script>
    <script type="module">
        // 引入spark-md5库
        // import { SparkMD5 } from './spark-md5.js';

        function calculateFileHash(file) {
            return new Promise((resolve, reject) => {
                const spark = new SparkMD5.ArrayBuffer();
                const fileReader = new FileReader();
                /*
                SparkMD5 是 MD5 算法的快速 md5 实现。此脚本基于 JKM md5 库，这是最快的算法。这最适合浏览器使用
                */
                fileReader.onload = (e) => {
                    spark.append(e.target.result); // Append array buffer
                    resolve(spark.end());
                };

                fileReader.onerror = () => {
                    reject('无法读取文件');
                };

                fileReader.readAsArrayBuffer(file);
            });
        }

        // 定义文件每片的分片大小 5MB
        const CHUNK_SIZE2 = 1024 * 1024 * 5;

        const cutFile2 = async (file) => {

            // 计算文件总分片数
            const chunkCount = Math.ceil(file.size / CHUNK_SIZE); // 分片向上取整，如：5.5片就要分6片
            console.log('文件总分片数：', chunkCount);

            console.log('\n文件分片中...！🚀如果文件是存储SSD固态硬盘中的话，读取速度会有非常大的提升哦！！\n\n');

            const chunks = [];
            // 获取文件分片每片的信息
            for (let i = 0; i < chunkCount; i++) {
                chunks.push(calculateFileHash(file, i, CHUNK_SIZE))
            }
            // return chunks;
            // 最后用Promise并行创建分片
            return await Promise.all(chunks);

            // 通过以上的尝试最后速度效果都差不多, 所以可以尝试用多线程worker的方式来处理。
        };

        // 使用示例
        document.querySelector('#file2').addEventListener('change', async (e) => {

            const file = e.target.files[0];
            console.time('耗时');

            const chunks = await cutFile2(file);
            document.querySelector('#chunks2').innerText = chunks

            console.log('分片结果：', chunks);

        });
    </script>
</body>

</html>